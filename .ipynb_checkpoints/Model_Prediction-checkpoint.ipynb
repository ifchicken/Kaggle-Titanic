{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.read_csv('data_clean.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Age               0\n",
      "Embarked          0\n",
      "PassengerId       0\n",
      "Pclass            0\n",
      "Sex               0\n",
      "Fare_bin          0\n",
      "Fare_class        0\n",
      "Family_group      0\n",
      "Title             0\n",
      "Deck              0\n",
      "Survived        418\n",
      "dtype: int64\n",
      "    Age  Embarked  PassengerId  Pclass  Sex  Fare_bin  Fare_class  \\\n",
      "0  22.0       2.0          1.0     2.0  1.0       0.0         0.0   \n",
      "1  38.0       0.0          2.0     0.0  0.0       3.0         2.0   \n",
      "2  26.0       2.0          3.0     2.0  0.0       0.0         0.0   \n",
      "3  35.0       2.0          4.0     0.0  0.0       3.0         2.0   \n",
      "4  35.0       2.0          5.0     2.0  1.0       0.0         0.0   \n",
      "\n",
      "   Family_group  Title  Deck  Survived  \n",
      "0           1.0    2.0   8.0       0.0  \n",
      "1           1.0    3.0   2.0       1.0  \n",
      "2           0.0    1.0   8.0       1.0  \n",
      "3           1.0    3.0   2.0       1.0  \n",
      "4           0.0    2.0   8.0       0.0  \n"
     ]
    }
   ],
   "source": [
    "##data[['Survived']] = data[['Survived']].apply(pd.to_numeric, errors='ignore')\n",
    "#data[['Age']] = data[['Age']].astype(int)\n",
    "data = data.astype('float32', casting='same_kind')\n",
    "print (data.isnull().sum() )\n",
    "print (data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1309 entries, 0 to 1308\n",
      "Data columns (total 11 columns):\n",
      "Age             1309 non-null float32\n",
      "Embarked        1309 non-null float32\n",
      "PassengerId     1309 non-null float32\n",
      "Pclass          1309 non-null float32\n",
      "Sex             1309 non-null float32\n",
      "Fare_bin        1309 non-null float32\n",
      "Fare_class      1309 non-null float32\n",
      "Family_group    1309 non-null float32\n",
      "Title           1309 non-null float32\n",
      "Deck            1309 non-null float32\n",
      "Survived        891 non-null float32\n",
      "dtypes: float32(11)\n",
      "memory usage: 56.3 KB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>mean</th>\n",
       "      <th>std</th>\n",
       "      <th>min</th>\n",
       "      <th>25%</th>\n",
       "      <th>50%</th>\n",
       "      <th>75%</th>\n",
       "      <th>max</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Age</th>\n",
       "      <td>1309.0</td>\n",
       "      <td>29.523966</td>\n",
       "      <td>13.425241</td>\n",
       "      <td>0.17</td>\n",
       "      <td>21.0</td>\n",
       "      <td>28.299999</td>\n",
       "      <td>36.5</td>\n",
       "      <td>80.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Embarked</th>\n",
       "      <td>1309.0</td>\n",
       "      <td>1.490451</td>\n",
       "      <td>0.816089</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>PassengerId</th>\n",
       "      <td>1309.0</td>\n",
       "      <td>655.000000</td>\n",
       "      <td>378.020081</td>\n",
       "      <td>1.00</td>\n",
       "      <td>328.0</td>\n",
       "      <td>655.000000</td>\n",
       "      <td>982.0</td>\n",
       "      <td>1309.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Pclass</th>\n",
       "      <td>1309.0</td>\n",
       "      <td>1.294882</td>\n",
       "      <td>0.837836</td>\n",
       "      <td>0.00</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Sex</th>\n",
       "      <td>1309.0</td>\n",
       "      <td>0.644003</td>\n",
       "      <td>0.478997</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare_bin</th>\n",
       "      <td>1309.0</td>\n",
       "      <td>1.100076</td>\n",
       "      <td>1.207738</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Fare_class</th>\n",
       "      <td>1309.0</td>\n",
       "      <td>1.057296</td>\n",
       "      <td>1.007883</td>\n",
       "      <td>-1.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Family_group</th>\n",
       "      <td>1309.0</td>\n",
       "      <td>0.333843</td>\n",
       "      <td>0.471765</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Title</th>\n",
       "      <td>1309.0</td>\n",
       "      <td>1.858671</td>\n",
       "      <td>0.722537</td>\n",
       "      <td>0.00</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Deck</th>\n",
       "      <td>1309.0</td>\n",
       "      <td>6.729565</td>\n",
       "      <td>2.454520</td>\n",
       "      <td>0.00</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.000000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>8.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Survived</th>\n",
       "      <td>891.0</td>\n",
       "      <td>0.383838</td>\n",
       "      <td>0.486592</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               count        mean         std   min    25%         50%    75%  \\\n",
       "Age           1309.0   29.523966   13.425241  0.17   21.0   28.299999   36.5   \n",
       "Embarked      1309.0    1.490451    0.816089  0.00    1.0    2.000000    2.0   \n",
       "PassengerId   1309.0  655.000000  378.020081  1.00  328.0  655.000000  982.0   \n",
       "Pclass        1309.0    1.294882    0.837836  0.00    1.0    2.000000    2.0   \n",
       "Sex           1309.0    0.644003    0.478997  0.00    0.0    1.000000    1.0   \n",
       "Fare_bin      1309.0    1.100076    1.207738 -1.00    0.0    1.000000    2.0   \n",
       "Fare_class    1309.0    1.057296    1.007883 -1.00    0.0    1.000000    2.0   \n",
       "Family_group  1309.0    0.333843    0.471765  0.00    0.0    0.000000    1.0   \n",
       "Title         1309.0    1.858671    0.722537  0.00    2.0    2.000000    2.0   \n",
       "Deck          1309.0    6.729565    2.454520  0.00    8.0    8.000000    8.0   \n",
       "Survived       891.0    0.383838    0.486592  0.00    0.0    0.000000    1.0   \n",
       "\n",
       "                 max  \n",
       "Age             80.0  \n",
       "Embarked         2.0  \n",
       "PassengerId   1309.0  \n",
       "Pclass           2.0  \n",
       "Sex              1.0  \n",
       "Fare_bin         3.0  \n",
       "Fare_class       3.0  \n",
       "Family_group     1.0  \n",
       "Title            3.0  \n",
       "Deck             8.0  \n",
       "Survived         1.0  "
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data = data.drop('Age', axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(891, 9) (418, 9)\n"
     ]
    }
   ],
   "source": [
    "X_train = data[0:891].drop(['PassengerId', 'Survived'], axis=1)\n",
    "y_train = data['Survived'][0:891]\n",
    "X_test = data[891:].drop(['PassengerId', 'Survived'], axis=1)\n",
    "print (X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#X_train.describe().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StandardScaler(copy=True, with_mean=True, with_std=True)"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "StandardScaler(copy=True, with_mean=True, with_std=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = scaler.transform(X_train)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#print (X_train.head())\n",
    "#print (X_test.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLEARN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# random forest 78%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy =  88.4399551066 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "#clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=5))\n",
    "clf = RandomForestClassifier(criterion='gini', \n",
    "                             n_estimators=1000,\n",
    "                             min_samples_split=12,\n",
    "                             min_samples_leaf=1,\n",
    "                             oob_score=True,\n",
    "                             random_state=1,\n",
    "                             n_jobs=-1) \n",
    "\n",
    "#AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=5))\n",
    "clf = clf.fit(X_train, y_train)\n",
    "    \n",
    "predict = clf.predict(X_train)\n",
    "acc = accuracy_score(predict, y_train)\n",
    "print(\"train accuracy = \", acc * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# AdaBoost 71%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy =  95.0617283951 %\n"
     ]
    }
   ],
   "source": [
    "clf = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(criterion=\"entropy\", max_depth=4))\n",
    "clf = clf.fit(X_train, y_train)\n",
    "predict = clf.predict(X_train)\n",
    "acc = accuracy_score(predict, y_train)\n",
    "print(\"train accuracy = \", acc * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# SVM, 69%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy =  85.746352413 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf = svm.SVC()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "predict = clf.predict(X_train)\n",
    "acc = accuracy_score(predict, y_train)\n",
    "print(\"train accuracy = \", acc * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# KNN, 60%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy =  84.1750841751 %\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "clf = KNeighborsClassifier()\n",
    "clf = clf.fit(X_train, y_train)\n",
    "predict = clf.predict(X_train)\n",
    "acc = accuracy_score(predict, y_train)\n",
    "print(\"train accuracy = \", acc * 100, \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     PassengerId  Survived\n",
      "891          892         0\n",
      "892          893         0\n",
      "893          894         1\n",
      "894          895         1\n",
      "895          896         1\n"
     ]
    }
   ],
   "source": [
    "y_test = clf.predict(X_test)\n",
    "y_test = y_test.astype(int).reshape(data.shape[0] - 891)\n",
    "output = pd.DataFrame({'PassengerId': data['PassengerId'][891:], 'Survived': y_test})\n",
    "print (output.head())\n",
    "output.to_csv('submit.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import flatten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mu = 0\n",
    "sigma = 0.1\n",
    "\n",
    "#weights\n",
    "weights = {\n",
    "    'fc1_w' : tf.Variable(tf.truncated_normal(shape=(9, 9), mean=mu, stddev=sigma)),\n",
    "    'fc2_w' : tf.Variable(tf.truncated_normal(shape=(9, 9), mean=mu, stddev=sigma)),\n",
    "    'fc3_w' : tf.Variable(tf.truncated_normal(shape=(9, 5), mean=mu, stddev=sigma)),\n",
    "    'fc4_w' : tf.Variable(tf.truncated_normal(shape=(5, 2), mean=mu, stddev=sigma)),\n",
    "}\n",
    "\n",
    "#output\n",
    "biases = {\n",
    "    'fc1_b' : tf.Variable(tf.zeros(9)),\n",
    "    'fc2_b' : tf.Variable(tf.zeros(9)), \n",
    "    'fc3_b' : tf.Variable(tf.zeros(5)),\n",
    "    'fc4_b' : tf.Variable(tf.zeros(2))    \n",
    "}\n",
    "\n",
    "def model(x, weights, biases, keep_prob):\n",
    "    fc1 = tf.matmul(x, weights['fc1_w']) + biases['fc1_b']\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    fc1 = tf.nn.dropout(fc1, keep_prob)\n",
    "\n",
    "    fc2 = tf.matmul(fc1, weights['fc2_w']) + biases['fc2_b']\n",
    "    fc2 = tf.nn.relu(fc2)\n",
    "    fc2 = tf.nn.dropout(fc2, keep_prob)    \n",
    "\n",
    "    fc3 = tf.matmul(fc2, weights['fc3_w']) + biases['fc3_b']\n",
    "    fc3 = tf.nn.relu(fc3)\n",
    "    fc3 = tf.nn.dropout(fc3, keep_prob)    \n",
    "    #output\n",
    "    fc4 = tf.matmul(fc3, weights['fc4_w']) + biases['fc4_b']\n",
    "    return fc4\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 700\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "x = tf.placeholder(tf.float32, (None, 9))\n",
    "y = tf.placeholder(tf.int32, (None))\n",
    "one_hot_y = tf.one_hot(y, 2)\n",
    "\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "rate = 0.00008\n",
    "logits = model(x, weights, biases, keep_prob)\n",
    "cross_entropy = tf.nn.softmax_cross_entropy_with_logits(logits = logits, labels=one_hot_y)\n",
    "loss_operation = tf.reduce_mean(cross_entropy)\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=rate)\n",
    "training_operation = optimizer.minimize(loss_operation)\n",
    "##\n",
    "correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(one_hot_y, 1))\n",
    "# tf.argmax(input, axis) return the index of max nmuber of row\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate(X_data, y_data):\n",
    "    num_examples = len(X_data)\n",
    "    total_accuracy=0\n",
    "    sess= tf.get_default_session()\n",
    "    for offset in range(0, num_examples, BATCH_SIZE):\n",
    "        batch_x, batch_y = X_data[offset:offset+BATCH_SIZE], y_data[offset:offset+BATCH_SIZE]\n",
    "        accuracy = sess.run(accuracy_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: 1.0})\n",
    "        total_accuracy += (accuracy * len(batch_x))\n",
    "    return total_accuracy / num_examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "\n",
      "EPOCH 1 ...\n",
      "Train Accuracy = 0.616\n",
      "\n",
      "EPOCH 51 ...\n",
      "Train Accuracy = 0.616\n",
      "\n",
      "EPOCH 101 ...\n",
      "Train Accuracy = 0.616\n",
      "\n",
      "EPOCH 151 ...\n",
      "Train Accuracy = 0.616\n",
      "\n",
      "EPOCH 201 ...\n",
      "Train Accuracy = 0.616\n",
      "\n",
      "EPOCH 251 ...\n",
      "Train Accuracy = 0.707\n",
      "\n",
      "EPOCH 301 ...\n",
      "Train Accuracy = 0.731\n",
      "\n",
      "EPOCH 351 ...\n",
      "Train Accuracy = 0.743\n",
      "\n",
      "EPOCH 401 ...\n",
      "Train Accuracy = 0.753\n",
      "\n",
      "EPOCH 451 ...\n",
      "Train Accuracy = 0.761\n",
      "\n",
      "EPOCH 501 ...\n",
      "Train Accuracy = 0.768\n",
      "\n",
      "EPOCH 551 ...\n",
      "Train Accuracy = 0.770\n",
      "\n",
      "EPOCH 601 ...\n",
      "Train Accuracy = 0.776\n",
      "\n",
      "EPOCH 651 ...\n",
      "Train Accuracy = 0.776\n",
      "\n",
      "Model saved\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "kp = 1.0\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    num_examples = len(X_train)\n",
    "    \n",
    "    print (\"Training...\")\n",
    "    print ()\n",
    "    for i in range(EPOCHS):\n",
    "        X_training, y_training = shuffle(X_train, y_train)\n",
    "        for offset in range(0, num_examples, BATCH_SIZE):\n",
    "            end = offset + BATCH_SIZE\n",
    "            batch_x, batch_y = X_training[offset: end], y_training[offset: end]\n",
    "            sess.run(training_operation, feed_dict={x: batch_x, y: batch_y, keep_prob: kp})\n",
    "            \n",
    "        train_accuracy = evaluate(X_training, y_training)    \n",
    "#        validation_accuracy = evaluate(X_valid, y_valid)\n",
    "        if i % 50 == 0:\n",
    "            print (\"EPOCH {} ...\".format(i+1))\n",
    "            print (\"Train Accuracy = {:.3f}\".format(train_accuracy))\n",
    "#        print (\"Validation Accuracy = {:.3f}\".format(validation_accuracy))\n",
    "            print ()\n",
    "        \n",
    "    saver.save(sess, './lenet')\n",
    "    print (\"Model saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_49 (Dense)                 (None, 9)             90          dense_input_15[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "dense_50 (Dense)                 (None, 9)             90          dense_49[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_51 (Dense)                 (None, 5)             50          dense_50[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_52 (Dense)                 (None, 1)             6           dense_51[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 236\n",
      "Trainable params: 236\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import keras \n",
    "from keras.models import Sequential \n",
    "from keras.layers import Dense, Activation, Flatten, Dropout\n",
    "\n",
    "# Initialising the NN\n",
    "model = Sequential()\n",
    "\n",
    "#layers\n",
    "#model.add(Dense(9, kernel_initializer = 'uniform', Activation = 'relu', input_dim = 9))\n",
    "model.add(Dense(9, activation = 'relu', input_dim = 9))\n",
    "model.add(Dense(9, activation = 'relu'))\n",
    "model.add(Dense(5, activation = 'relu'))\n",
    "model.add(Dense(1, activation = 'softmax'))\n",
    "\n",
    "# summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "You are passing a target array of shape (891, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils.np_utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-8c2e6f542533>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mX_train2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0my_train2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m200\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/Users/ifchicken/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/ifchicken/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1114\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1115\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1116\u001b[0;31m             batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1117\u001b[0m         \u001b[0;31m# prepare validation data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1118\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ifchicken/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_batch_axis, batch_size)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                           in zip(y, sample_weights, class_weights, self.sample_weight_modes)]\n\u001b[1;32m   1041\u001b[0m         \u001b[0mcheck_array_lengths\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m         \u001b[0mcheck_loss_and_target_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_functions\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minternal_output_shapes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1043\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstateful\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/ifchicken/miniconda3/envs/carnd-term1/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mcheck_loss_and_target_compatibility\u001b[0;34m(targets, losses, output_shapes)\u001b[0m\n\u001b[1;32m    208\u001b[0m                 raise ValueError(\n\u001b[1;32m    209\u001b[0m                     \u001b[0;34m'You are passing a target array of shape '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m                     \u001b[0;34m' while using as loss `categorical_crossentropy`. '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m                     \u001b[0;34m'`categorical_crossentropy` expects '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m                     \u001b[0;34m'targets to be binary matrices (1s and 0s) '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: You are passing a target array of shape (891, 1) while using as loss `categorical_crossentropy`. `categorical_crossentropy` expects targets to be binary matrices (1s and 0s) of shape (samples, classes). If your targets are integer classes, you can convert them to the expected format via:\n```\nfrom keras.utils.np_utils import to_categorical\ny_binary = to_categorical(y_int)\n```\n\nAlternatively, you can use the loss function `sparse_categorical_crossentropy` instead, which does expect integer targets."
     ]
    }
   ],
   "source": [
    "# Compiling the NN\n",
    "#model.compile(optimizer = 'adam', loss = 'mse')\n",
    "model.compile(optimizer = 'adam', loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Train the NN\n",
    "# remember to use np.array to make it work\n",
    "X_train2 = np.array(X_train)\n",
    "y_train2 = np.array(y_train)\n",
    "model.fit(X_train2, y_train2, batch_size = 32, nb_epoch = 200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:carnd-term1]",
   "language": "python",
   "name": "conda-env-carnd-term1-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
